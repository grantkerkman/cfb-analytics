{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a79e7d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32041476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Chrome browser instance\n",
    "browser = Browser('chrome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1947498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Years for analysis\n",
    "years = np.arange(2000, 2023, 1)\n",
    "\n",
    "# Create lists for the urls\n",
    "offense_urls = []\n",
    "defense_urls = []\n",
    "standings_urls = []\n",
    "\n",
    "# Iterate through the years to make urls\n",
    "for year in years:\n",
    "    \n",
    "    # Generate the base url\n",
    "    offense_url = f'https://www.sports-reference.com/cfb/years/{year}-team-offense.html'\n",
    "    defense_url = f'https://www.sports-reference.com/cfb/years/{year}-team-defense.html'\n",
    "    standings_url = f'https://www.sports-reference.com/cfb/years/{year}-standings.html'\n",
    "    \n",
    "    # Append the urls to the respective lists\n",
    "    offense_urls.append(offense_url)\n",
    "    defense_urls.append(defense_url)\n",
    "    standings_urls.append(standings_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "229d6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Offense\n",
    "####################\n",
    "\n",
    "# Script to automate browsing\n",
    "combined_data = []\n",
    "\n",
    "# Visit each year the url list\n",
    "for url in offense_urls:  \n",
    "    \n",
    "    # Visit the page and create the soup object\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Scrape the offensive table from the page\n",
    "    tables = soup.find_all('tbody')\n",
    "    table = tables[-1]\n",
    "    \n",
    "    # Execute JavaScript to stop further page loading\n",
    "    browser.execute_script(\"window.stop();\")\n",
    "    \n",
    "    # Extract data from the table\n",
    "    # Create an empty list\n",
    "    data_list = []\n",
    "    \n",
    "    # Get the year from the page\n",
    "    year = soup.find('h1').find('span').text\n",
    "    \n",
    "    # Iterate through the rows in the table\n",
    "    for row in table.find_all('tr'):\n",
    "        \n",
    "        # Check if the row belongs to the <tfoot> section and exclude\n",
    "        if row.find_parent('tfoot'):\n",
    "            continue\n",
    "        \n",
    "        # Empty list for the row data\n",
    "        row_data = []\n",
    "        \n",
    "        # Append the year to the row_data\n",
    "        row_data.append(year)\n",
    "\n",
    "        # Iterate through the <td> tags for each cell data point in the current row\n",
    "        for cell in row.find_all('td'):\n",
    "            \n",
    "            row_data.append(cell.get_text())\n",
    "\n",
    "        # Append the row_data list to the data_list\n",
    "        data_list.append(row_data)\n",
    "            \n",
    "    combined_data.extend(data_list)\n",
    "\n",
    "## Create a Pandas DataFrame by using the list of rows and a list of the column names\n",
    "columns = ['year', 'school', 'games', 'points', 'passing_cmp', 'passing_att', 'passing_pct', 'passing_yds', 'passing_td', \n",
    "           'rushing_att', 'rushing_yds', 'rushing_avg', 'rushing_td', \n",
    "           'total_plays', 'total_yds', 'total_avg', \n",
    "           'first_down_pass', 'first_down_rush', 'first_down_pen', 'first_down_total',\n",
    "           'penalties', 'penalty_yds', \n",
    "           'fumbles', 'intceptions', 'turnovers' \n",
    "]\n",
    "\n",
    "all_offense_df = pd.DataFrame(combined_data, columns=columns)\n",
    "\n",
    "# Print the Files to a .csv\n",
    "all_offense_df.to_csv('../resources/all_offense.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffaa7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Defense\n",
    "####################\n",
    "\n",
    "# Script to automate browsing\n",
    "combined_data = []\n",
    "\n",
    "# Visit each year the url list\n",
    "for url in defense_urls:  \n",
    "    \n",
    "    # Visit the page and create the soup object\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Scrape the offensive table from the page\n",
    "    tables = soup.find_all('tbody')\n",
    "    table = tables[-1]\n",
    "    \n",
    "    # Execute JavaScript to stop further page loading\n",
    "    browser.execute_script(\"window.stop();\")\n",
    "    \n",
    "    # Extract data from the table\n",
    "    # Create an empty list\n",
    "    data_list = []\n",
    "    \n",
    "    # Get the year from the page\n",
    "    year = soup.find('h1').find('span').text\n",
    "    \n",
    "    # Iterate through the rows in the table\n",
    "    for row in table.find_all('tr'):\n",
    "        \n",
    "        # Check if the row belongs to the <tfoot> section and exclude\n",
    "        if row.find_parent('tfoot'):\n",
    "            continue\n",
    "        \n",
    "        # Empty list for the row data\n",
    "        row_data = []\n",
    "        \n",
    "        # Append the year to the row_data\n",
    "        row_data.append(year)\n",
    "\n",
    "        # Iterate through the <td> tags for each cell data point in the current row\n",
    "        for cell in row.find_all('td'):\n",
    "            \n",
    "            row_data.append(cell.get_text())\n",
    "\n",
    "        # Append the row_data list to the data_list\n",
    "        data_list.append(row_data)\n",
    "            \n",
    "    combined_data.extend(data_list)\n",
    "\n",
    "## Create a Pandas DataFrame by using the list of rows and a list of the column names\n",
    "columns = ['year', 'school', 'games', 'points', 'passing_cmp', 'passing_att', 'passing_pct', 'passing_yds', 'passing_td', \n",
    "           'rushing_att', 'rushing_yds', 'rushing_avg', 'rushing_td', \n",
    "           'total_plays', 'total_yds', 'total_avg', \n",
    "           'first_down_pass', 'first_down_rush', 'first_down_pen', 'first_down_total',\n",
    "           'penalties', 'penalty_yds', \n",
    "           'fumbles', 'intceptions', 'turnovers' \n",
    "]\n",
    "\n",
    "all_defense_df = pd.DataFrame(combined_data, columns=columns)\n",
    "\n",
    "# Print the Files to a .csv\n",
    "all_defense_df.to_csv('../resources/all_defense.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9af705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Standings\n",
    "####################\n",
    "\n",
    "# Script to automate browsing\n",
    "combined_data = []\n",
    "\n",
    "# Visit each year the url list\n",
    "for url in standings_urls:  \n",
    "    \n",
    "    # Visit the page and create the soup object\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Scrape the offensive table from the page\n",
    "    tables = soup.find_all('tbody')\n",
    "    table = tables[-1]\n",
    "    \n",
    "    # Execute JavaScript to stop further page loading\n",
    "    browser.execute_script(\"window.stop();\")\n",
    "    \n",
    "    # Extract data from the table\n",
    "    # Create an empty list\n",
    "    data_list = []\n",
    "    \n",
    "    # Get the year from the page\n",
    "    year = soup.find('h1').find('span').text\n",
    "    \n",
    "    # Iterate through the rows in the table\n",
    "    for row in table.find_all('tr'):\n",
    "        \n",
    "        # Check if the row belongs to the <tfoot> section and exclude\n",
    "        if row.find_parent('tfoot'):\n",
    "            continue\n",
    "        \n",
    "        # Empty list for the row data\n",
    "        row_data = []\n",
    "        \n",
    "        # Append the year to the row_data\n",
    "        row_data.append(year)\n",
    "\n",
    "        # Iterate through the <td> tags for each cell data point in the current row\n",
    "        for cell in row.find_all('td'):\n",
    "            \n",
    "            row_data.append(cell.get_text())\n",
    "\n",
    "        # Append the row_data list to the data_list\n",
    "        data_list.append(row_data)\n",
    "            \n",
    "    combined_data.extend(data_list)\n",
    "\n",
    "## Create a Pandas DataFrame by using the list of rows and a list of the column names\n",
    "columns = ['year', 'school', 'conference', \n",
    "           'wins', 'losses', 'winning_pct', \n",
    "           'conf_wins', 'conf_losses', 'conf_winning_pct',\n",
    "           'ppg_offense', 'ppg_defense', \n",
    "           'SRS', 'SOS', 'ap_pre', 'ap_high', 'ap_post', 'notes' \n",
    "]\n",
    "\n",
    "standings_df = pd.DataFrame(combined_data, columns=columns)\n",
    "\n",
    "# Print the Files to a .csv\n",
    "standings_df.to_csv('../resources/standings.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
