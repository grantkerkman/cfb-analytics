{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a902b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "from splinter import Browser\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a47db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the schools list (###REFERENCE TO GITHUB WHERE THAT SCRIPT IS###)\n",
    "school_list_df = pd.read_csv('resources/schools.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92eeef33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the school to analyze (capitalize first letter of each word) Nebraska\n",
      "Starting Year (YYYY format)2000\n",
      "Ending Year (type in same year for single year analysis)2022\n",
      "You selected Nebraska from years 2000 through 2022\n"
     ]
    }
   ],
   "source": [
    "# Get user inputs for school and year\n",
    "school = input(f'Select the school to analyze (capitalize first letter of each word) ')\n",
    "\n",
    "# Years for analysis\n",
    "year_i = input(f'Starting Year (YYYY format)')\n",
    "year_f = input(f'Ending Year (type in same year for single year analysis)')\n",
    "\n",
    "# Generate the url list\n",
    "base_url = 'https://www.sports-reference.com'\n",
    "\n",
    "# Create the schools url portion\n",
    "school_url = school_list_df.query(\"school == @school\")[\"link\"].values[0]\n",
    "\n",
    "# Create the years integers\n",
    "year_i = int(year_i)\n",
    "year_f = int(year_f)\n",
    "years = np.arange(year_i, 1+ year_f, 1)\n",
    "\n",
    "# Create a list for the urls\n",
    "urls = []\n",
    "\n",
    "for year in years:\n",
    "    url = f'{base_url}{school_url}{year}/gamelog/'\n",
    "    urls.append(url)\n",
    "    \n",
    "print(f'You selected {school} from years {year_i} through {year_f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d97470ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Chrome browser instance\n",
    "browser = Browser('chrome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c2d91ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to automate browsing for Offense\n",
    "combined_data = []\n",
    "\n",
    "# Visit each year the url list\n",
    "for url in urls:  \n",
    "    \n",
    "    # Visit the page and create the soup object\n",
    "    browser.visit(url)\n",
    "    browser.is_element_present_by_id('offense', wait_time=5)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Scrape the offensive table from the page\n",
    "    table = soup.find('table', {'class': 'sortable', 'id': 'offense'})\n",
    "    \n",
    "    # Execute JavaScript to stop further page loading\n",
    "    browser.execute_script(\"window.stop();\")\n",
    "    \n",
    "    # Extract data from the table\n",
    "    # Create an empty list\n",
    "    data_list = []\n",
    "    \n",
    "    # Iterate through the rows in the table\n",
    "    for row in table.find_all('tr'):\n",
    "        \n",
    "        # Check if the row belongs to the <tfoot> section and exclude\n",
    "        if row.find_parent('tfoot'):\n",
    "            continue\n",
    "        \n",
    "        # Empty list for the row data\n",
    "        row_data = []\n",
    "\n",
    "        # Iterate through the <td> tags for each cell data point in the current row\n",
    "        for cell in row.find_all('td'):\n",
    "            \n",
    "            row_data.append(cell.get_text())\n",
    "\n",
    "        # Append the row_data list to the data_list\n",
    "        data_list.append(row_data)\n",
    "            \n",
    "    combined_data.extend(data_list)\n",
    "\n",
    "## Create a Pandas DataFrame by using the list of rows and a list of the column names\n",
    "columns = ['date', 'home_away', 'opponent', 'score', 'passing_cmp', \n",
    "           'passing_att', 'passing_pct', 'passing_yds', 'passing_td', 'rushing_att', \n",
    "           'rushing_yds', 'rushing_avg', 'rushing_td', 'total_plays', 'total_yds', \n",
    "           'total_avg', 'first_down_pass', 'first_down_rush', 'first_down_pen', 'first_down_total',\n",
    "           'penalties', 'penalty_yds', 'fumbles', 'intceptions', 'turnovers'\n",
    "]\n",
    "\n",
    "offense_df = pd.DataFrame(combined_data, columns=columns)\n",
    "\n",
    "# Print the Files to a .csv\n",
    "offense_df.to_csv('resources/offense.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1a5e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to automate browsing for Defense\n",
    "combined_data = []\n",
    "\n",
    "# Visit each year the url list\n",
    "for url in urls:  \n",
    "    # Visit the page and create the soup object\n",
    "    browser.visit(url)\n",
    "    browser.is_element_present_by_id('defense', wait_time=5)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Scrape the defensive table\n",
    "    table = soup.find('table', {'class': 'sortable', 'id': 'defense'})\n",
    "    \n",
    "    # Extract data from the table\n",
    "    # Create an empty list\n",
    "    data_list = []\n",
    "    \n",
    "    # Iterate through the rows in the table\n",
    "    for row in table.find_all('tr'): \n",
    "        \n",
    "        # Check if the row belongs to the <tfoot> section and exclude\n",
    "        if row.find_parent('tfoot'):\n",
    "            continue\n",
    "        \n",
    "        # Empty list for the row data\n",
    "        row_data = []\n",
    "\n",
    "        # Iterate through the <td> tags for each cell data point in the current row\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.get_text())\n",
    "\n",
    "        # Append the row_data list to the data_list\n",
    "        data_list.append(row_data)\n",
    "            \n",
    "    combined_data.extend(data_list)\n",
    "        \n",
    "## Create a Pandas DataFrame by using the list of rows and a list of the column names\n",
    "columns = ['date', 'home_away', 'opponent', 'score', 'passing_cmp', \n",
    "           'passing_att', 'passing_pct', 'passing_yds', 'passing_td', 'rushing_att', \n",
    "           'rushing_yds', 'rushing_avg', 'rushing_td', 'total_plays', 'total_yds', \n",
    "           'total_avg', 'first_down_pass', 'first_down_rush', 'first_down_pen', 'first_down_total',\n",
    "           'penalties', 'penalty_yds', 'fumbles', 'intceptions', 'turnovers'\n",
    "]\n",
    "\n",
    "defense_df = pd.DataFrame(combined_data, columns=columns)\n",
    "\n",
    "# Print the Files to a .csv\n",
    "defense_df.to_csv('resources/defense.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023e3762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
